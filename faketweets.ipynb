{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faketweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqL+mAfD6GBlK7pkSKnVll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durgaprasadgaddam/Faketweets/blob/master/faketweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oDZXXYVH2XD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words =nltk.corpus.stopwords.words('english')\n",
        "import nltk\n",
        "ps =nltk.PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HsnP-CjH5UI",
        "colab_type": "code",
        "outputId": "8fe68fc1-e354-480f-c1f1-2f14749afc8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "tweets=pd.read_csv('/content/tweets.csv')\n",
        "tweets.head(5)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                                                                              text\n",
              "0   0  ...                                                                Just happened a terrible car crash\n",
              "1   2  ...                                  Heard about #earthquake is different cities, stay safe everyone.\n",
              "2   3  ...  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\n",
              "3   9  ...                                                          Apocalypse lighting. #Spokane #wildfires\n",
              "4  11  ...                                                     Typhoon Soudelor kills 28 in China and Taiwan\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1HbMW4v6bu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYQmp-OFIRmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The tweets dataset contains {} rows and {} columns\".format(len(tweets),len(tweets.columns)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbhmx6aYIpWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"in the total {} records for the column id contains {} null values \".format(len(tweets),tweets['id'].isnull().sum()))\n",
        "print (\"in the total {} records for the column keyword contains {} null values \".format(len(tweets),tweets['keyword'].isnull().sum()))\n",
        "print (\"in the total {} records for the column location contains {} null values \".format(len(tweets),tweets['location'].isnull().sum()))\n",
        "print (\"in the total {} records for the column text contains {} null values \".format(len(tweets),tweets['text'].isnull().sum()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRzz7Ml_KKIl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Cleaning tweets text\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEZgazznKTS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string \n",
        "string.punctuation\n",
        "def clean_data(text):\n",
        "   text_output= \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "   tokenized_text = re.findall('\\w+',text_output)\n",
        "   no_stop_words = [word for word in tokenized_text if word not in stop_words]\n",
        "   stem_text=[ps.stem(word) for word in no_stop_words]\n",
        "   return stem_text\n",
        "  \n",
        "tweets['clean_text'] = tweets['text'].apply(lambda x : clean_data(x))\n",
        "\n",
        "\n",
        "tweets.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RSBOB_2X_5u",
        "colab_type": "text"
      },
      "source": [
        "Lemantizing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTjQXD7BYFGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wn =nltk.WordNetLemmatizer()\n",
        "pd.set_option('display.max_colwidth',100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHKe6jPCYbxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('wordnet')\n",
        "def word_lematizer(filtered_no_Stop):\n",
        "  stem_text=[\" \".join(wn.lemmatize(word) for word in filtered_no_Stop) ]\n",
        "  return stem_text\n",
        "tweets['lematized_data'] =tweets['text_without_stop'].apply(lambda x : word_lematizer(x))\n",
        "tweets.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urHoiJo4kNi-",
        "colab_type": "text"
      },
      "source": [
        "Vectorizing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZGno2LQ1-ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "ngram_vect =CountVectorizer()\n",
        "xcounts = ngram_vect.fit_transform(tweets['clean_text'])\n",
        "print(xcounts.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jayfPm4B2pLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_counts = cv.fit_transform(tweets['lematized_data'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}